{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "otherwise-investigation",
   "metadata": {},
   "source": [
    "# Chest X-ray Abnormalities Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-skirt",
   "metadata": {},
   "source": [
    "**ECE-GY 9123 | Deep Learning**\n",
    "\n",
    "**Spring 2021**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-circulation",
   "metadata": {},
   "source": [
    "#### TEAM MEMBERS\n",
    "\n",
    "**Kunwar Srivastav** (kss519)\n",
    "\n",
    "**Sagar Patel** (sp5894)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-baltimore",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Dataset Preparation](#Dataset-Preparation)\n",
    "- [Installation](#Installation)\n",
    "- [Exploratory Data Analysis: Distribution between Normal and Abnormal Classes](#Exploratory-Data-Analysis:-Distribution-between-Normal-and-Abnormal-Classes)\n",
    "- [Data Visualization and Augmentation](#Data-Visualization-and-Augmentation)\n",
    "- [Defining the CNN Models](#Defining-the-CNN-Models)\n",
    "- Training utils\n",
    "- Training Scripts\n",
    "- Prediction on Validation and Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-spare",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-providence",
   "metadata": {},
   "source": [
    "Preprocessing X-ray image format (dicom) into normal format is already done by kaggle user [xhulu](https://www.kaggle.com/xhlulu) and can be accessed from here:\n",
    "\n",
    "> [Multiple Preprocessed Datasets](https://www.kaggle.com/c/vinbigdata-chest-xray-abnormalities-detection/discussion/207955)\n",
    "\n",
    "Here, we will be using the dataset [VinBigData Chest X-ray Resized PNG (256 x 256)](https://www.kaggle.com/xhlulu/vinbigdata-chest-xray-resized-png-256x256) to skip the preprocessing and focus on the modeling part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-stewart",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-balloon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-double",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import tools, subplots\n",
    "import plotly.offline as py\n",
    "\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_dark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-houston",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-junction",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-anthropology",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-newsletter",
   "metadata": {},
   "source": [
    "To install detectron2, we can follow the given [installation instruction](https://github.com/facebookresearch/detectron2/blob/master/INSTALL.md). Note that we will need to know the right version of `CUDA` and `pytorch` to install `detectron2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-motion",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyyaml==5.1\n",
    "!pip install torch==1.7.1 torchvision==0.8.2\n",
    "\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "!gcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-texas",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch-pfn-extras timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-arrival",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "assert torch.__version__.startswith(\"1.7\")\n",
    "\n",
    "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu102/torch1.7/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-witness",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import yaml\n",
    "\n",
    "def save_yaml(filepath: str, content: Any, width: int = 120):\n",
    "    with open(filepath, \"w\") as f:\n",
    "        yaml.dump(content, f, width=width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Any, Tuple, Union, List\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Flags:\n",
    "    # General\n",
    "    debug: bool = True\n",
    "    outdir: str = \"results/det\"\n",
    "    device: str = \"cuda:0\"\n",
    "\n",
    "    # Data config\n",
    "    imgdir_name: str = \"vinbigdata-chest-xray-resized-png-256x256\"\n",
    "    \n",
    "    # split_mode: str = \"all_train\"  # all_train or valid20\n",
    "    seed: int = 111\n",
    "    target_fold: int = 0  # 0~4\n",
    "    label_smoothing: float = 0.0\n",
    "    \n",
    "    # Model config\n",
    "    model_name: str = \"resnet18\"\n",
    "    model_mode: str = \"normal\"  # normal, cnn_fixed supported\n",
    "    \n",
    "    # Training config\n",
    "    epoch: int = 20\n",
    "    batchsize: int = 8\n",
    "    valid_batchsize: int = 16\n",
    "    num_workers: int = 4\n",
    "    snapshot_freq: int = 5\n",
    "    ema_decay: float = 0.999  # negative value is to inactivate ema.\n",
    "    scheduler_type: str = \"\"\n",
    "    scheduler_kwargs: Dict[str, Any] = field(default_factory=lambda: {})\n",
    "    scheduler_trigger: List[Union[int, str]] = field(default_factory=lambda: [1, \"iteration\"])\n",
    "    aug_kwargs: Dict[str, Dict[str, Any]] = field(default_factory=lambda: {})\n",
    "    mixup_prob: float = -1.0  # Apply mixup augmentation when positive value is set.\n",
    "\n",
    "    def update(self, param_dict: Dict) -> \"Flags\":\n",
    "        # Overwrite by `param_dict`\n",
    "        for key, value in param_dict.items():\n",
    "            if not hasattr(self, key):\n",
    "                raise ValueError(f\"[ERROR] Unexpected key for flag = {key}\")\n",
    "            setattr(self, key, value)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "flags_dict = {\n",
    "    \"debug\": False,  # Change to True for fast debug run!\n",
    "    \"outdir\": \"results/tmp_debug\",\n",
    "    \n",
    "    # For the Data\n",
    "    \"imgdir_name\": \"vinbigdata-chest-xray-resized-png-256x256\",\n",
    "    \n",
    "    # For the Model\n",
    "    \"model_name\": \"resnet18\",\n",
    "    \n",
    "    # For Training the Model\n",
    "    \"num_workers\": 4,\n",
    "    \"epoch\": 15,\n",
    "    \"batchsize\": 8,\n",
    "    \"scheduler_type\": \"CosineAnnealingWarmRestarts\",\n",
    "    \"scheduler_kwargs\": {\"T_0\": 28125},  # 15000 * 15 epoch // (batchsize=8)\n",
    "    \"scheduler_trigger\": [1, \"iteration\"],\n",
    "    \"aug_kwargs\": {\n",
    "        \"HorizontalFlip\": {\"p\": 0.5},\n",
    "        \"ShiftScaleRotate\": {\"scale_limit\": 0.15, \"rotate_limit\": 10, \"p\": 0.5},\n",
    "        \"RandomBrightnessContrast\": {\"p\": 0.5},\n",
    "        \"CoarseDropout\": {\"max_holes\": 8, \"max_height\": 25, \"max_width\": 25, \"p\": 0.5},\n",
    "        \"Blur\": {\"blur_limit\": [3, 7], \"p\": 0.5},\n",
    "        \"Downscale\": {\"scale_min\": 0.25, \"scale_max\": 0.9, \"p\": 0.3},\n",
    "        \"RandomGamma\": {\"gamma_limit\": [80, 120], \"p\": 0.6},\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "print(\"torch\", torch.__version__)\n",
    "flags = Flags().update(flags_dict)\n",
    "print(\"flags\", flags)\n",
    "debug = flags.debug\n",
    "outdir = Path(flags.outdir)\n",
    "os.makedirs(str(outdir), exist_ok=True)\n",
    "flags_dict = dataclasses.asdict(flags)\n",
    "save_yaml(str(outdir / \"flags.yaml\"), flags_dict)\n",
    "\n",
    "# Read the data\n",
    "inputdir = Path(\"/kaggle/input\")\n",
    "datadir = inputdir / \"vinbigdata-chest-xray-abnormalities-detection\"\n",
    "imgdir = inputdir / flags.imgdir_name\n",
    "\n",
    "# Read in the data CSV files\n",
    "train = pd.read_csv(datadir / \"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-masters",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis: Distribution between Normal and Abnormal Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-cutting",
   "metadata": {},
   "source": [
    "We need to check how many 'normal' classes exist in the training data. It is classified as `class_name = No finding` and `class_id = 14`.\n",
    "\n",
    "However, there was an exception where `rad_id` is different for the same Image ID `50a418190bc3fb1ef1633bf9678929b3` as shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-aviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.query(\"image_id == '50a418190bc3fb1ef1633bf9678929b3'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-cheat",
   "metadata": {},
   "source": [
    "Is there an image that three radiologist's have a different opinion?\n",
    "\n",
    "Possibly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-lawrence",
   "metadata": {},
   "source": [
    "Let us check the number of \"No finding\" annotations for each image, if the opinions are in complete agreement the number of \"No finding\" annotations should be -\n",
    "\n",
    "0: Abnormal (all radiologists do not think that this is normal)\n",
    "\n",
    "1: Normal (all the radiologists unanimously agree that it is normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-inside",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_normal_df = train.groupby(\"image_id\")[\"class_id\"].agg(lambda s: (s == 14).sum()).reset_index().rename({\"class_id\": \"num_normal_annotations\"}, axis=1)\n",
    "is_normal_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-ethiopia",
   "metadata": {},
   "source": [
    "We could confirm that always 3 radiologists opinions match for normal - abnormal diagnosis.\n",
    "\n",
    "**NOTE:** We noticed that it does not apply for the other classes. i.e., 3 radiologists opinions sometimes do not match for the other class of thoracic abnormalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-chemistry",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_normal_anno_counts = is_normal_df[\"num_normal_annotations\"].value_counts()\n",
    "num_normal_anno_counts.plot(kind=\"bar\")\n",
    "plt.title(\"The number of 'No finding' annotations in each image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-bracket",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_normal_anno_counts_df = num_normal_anno_counts.reset_index()\n",
    "num_normal_anno_counts_df[\"name\"] = num_normal_anno_counts_df[\"index\"].map({0: \"Abnormal\", 3: \"Normal\"})\n",
    "num_normal_anno_counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-treatment",
   "metadata": {},
   "source": [
    "So almost 70% of the data is actually \"Normal\" X-ray images.\n",
    "\n",
    "Only 30% of the images need thoracic abnormality location detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-filename",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.pie(num_normal_anno_counts_df, values=\"num_normal_annotations\", names=\"name\", title=\"Normal/Abnormal ratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-anniversary",
   "metadata": {},
   "source": [
    "## Data Visualization and Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-candidate",
   "metadata": {},
   "source": [
    "When we train CNN models, image augmentation is important to avoid model to overfit.\n",
    "\n",
    "We will demonstrate some examples to use Albumentations to run image augmentation very easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-frost",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from detectron2.structures import BoxMode\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generous-visibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vinbigdata_dicts(\n",
    "    imgdir: Path,\n",
    "    train_df: pd.DataFrame,\n",
    "    train_data_type: str = \"original\",\n",
    "    use_cache: bool = True,\n",
    "    debug: bool = True,\n",
    "    target_indices: Optional[np.ndarray] = None,\n",
    "):\n",
    "    debug_str = f\"_debug{int(debug)}\"\n",
    "    train_data_type_str = f\"_{train_data_type}\"\n",
    "    cache_path = Path(\".\") / f\"dataset_dicts_cache{train_data_type_str}{debug_str}.pkl\"\n",
    "    if not use_cache or not cache_path.exists():\n",
    "        print(\"Creating data...\")\n",
    "        train_meta = pd.read_csv(imgdir / \"train_meta.csv\")\n",
    "        if debug:\n",
    "            train_meta = train_meta.iloc[:500]  # For debug....\n",
    "\n",
    "        # Load 1 image to get image size.\n",
    "        image_id = train_meta.loc[0, \"image_id\"]\n",
    "        image_path = str(imgdir / \"train\" / f\"{image_id}.png\")\n",
    "        image = cv2.imread(image_path)\n",
    "        resized_height, resized_width, ch = image.shape\n",
    "        print(f\"image shape: {image.shape}\")\n",
    "\n",
    "        dataset_dicts = []\n",
    "        for index, train_meta_row in tqdm(train_meta.iterrows(), total=len(train_meta)):\n",
    "            record = {}\n",
    "\n",
    "            image_id, height, width = train_meta_row.values\n",
    "            filename = str(imgdir / \"train\" / f\"{image_id}.png\")\n",
    "            record[\"file_name\"] = filename\n",
    "            record[\"image_id\"] = image_id\n",
    "            record[\"height\"] = resized_height\n",
    "            record[\"width\"] = resized_width\n",
    "            objs = []\n",
    "            for index2, row in train_df.query(\"image_id == @image_id\").iterrows():\n",
    "                # print(row)\n",
    "                # print(row[\"class_name\"])\n",
    "                # class_name = row[\"class_name\"]\n",
    "                class_id = row[\"class_id\"]\n",
    "                if class_id == 14:\n",
    "                    # It is \"No finding\"\n",
    "                    # This annotator does not find anything, skip.\n",
    "                    pass\n",
    "                else:\n",
    "                    # bbox_original = [int(row[\"x_min\"]), int(row[\"y_min\"]), int(row[\"x_max\"]), int(row[\"y_max\"])]\n",
    "                    h_ratio = resized_height / height\n",
    "                    w_ratio = resized_width / width\n",
    "                    bbox_resized = [\n",
    "                        int(row[\"x_min\"]) * w_ratio,\n",
    "                        int(row[\"y_min\"]) * h_ratio,\n",
    "                        int(row[\"x_max\"]) * w_ratio,\n",
    "                        int(row[\"y_max\"]) * h_ratio,\n",
    "                    ]\n",
    "                    obj = {\n",
    "                        \"bbox\": bbox_resized,\n",
    "                        \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "                        \"category_id\": class_id,\n",
    "                    }\n",
    "                    objs.append(obj)\n",
    "            record[\"annotations\"] = objs\n",
    "            dataset_dicts.append(record)\n",
    "        with open(cache_path, mode=\"wb\") as f:\n",
    "            pickle.dump(dataset_dicts, f)\n",
    "\n",
    "    print(f\"Load from cache {cache_path}\")\n",
    "    with open(cache_path, mode=\"rb\") as f:\n",
    "        dataset_dicts = pickle.load(f)\n",
    "    if target_indices is not None:\n",
    "        dataset_dicts = [dataset_dicts[i] for i in target_indices]\n",
    "    return dataset_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-thought",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vinbigdata_dicts_test(\n",
    "    imgdir: Path, test_meta: pd.DataFrame, use_cache: bool = True, debug: bool = True,\n",
    "):\n",
    "    debug_str = f\"_debug{int(debug)}\"\n",
    "    cache_path = Path(\".\") / f\"dataset_dicts_cache_test{debug_str}.pkl\"\n",
    "    if not use_cache or not cache_path.exists():\n",
    "        print(\"Creating data...\")\n",
    "        # test_meta = pd.read_csv(imgdir / \"test_meta.csv\")\n",
    "        if debug:\n",
    "            test_meta = test_meta.iloc[:500]  # For debug....\n",
    "\n",
    "        # Load 1 image to get image size.\n",
    "        image_id = test_meta.loc[0, \"image_id\"]\n",
    "        image_path = str(imgdir / \"test\" / f\"{image_id}.png\")\n",
    "        image = cv2.imread(image_path)\n",
    "        resized_height, resized_width, ch = image.shape\n",
    "        print(f\"image shape: {image.shape}\")\n",
    "\n",
    "        dataset_dicts = []\n",
    "        for index, test_meta_row in tqdm(test_meta.iterrows(), total=len(test_meta)):\n",
    "            record = {}\n",
    "\n",
    "            image_id, height, width = test_meta_row.values\n",
    "            filename = str(imgdir / \"test\" / f\"{image_id}.png\")\n",
    "            record[\"file_name\"] = filename\n",
    "            # record[\"image_id\"] = index\n",
    "            record[\"image_id\"] = image_id\n",
    "            record[\"height\"] = resized_height\n",
    "            record[\"width\"] = resized_width\n",
    "            # objs = []\n",
    "            # record[\"annotations\"] = objs\n",
    "            dataset_dicts.append(record)\n",
    "        with open(cache_path, mode=\"wb\") as f:\n",
    "            pickle.dump(dataset_dicts, f)\n",
    "\n",
    "    print(f\"Load from cache {cache_path}\")\n",
    "    with open(cache_path, mode=\"rb\") as f:\n",
    "        dataset_dicts = pickle.load(f)\n",
    "    return dataset_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-archive",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Referenced `chainer.dataset.DatasetMixin` to work with pytorch Dataset.\n",
    "\"\"\"\n",
    "import numpy\n",
    "import six\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "\n",
    "class DatasetMixin(Dataset):\n",
    "\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns an example or a sequence of examples.\"\"\"\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "        if isinstance(index, slice):\n",
    "            current, stop, step = index.indices(len(self))\n",
    "            return [self.get_example_wrapper(i) for i in\n",
    "                    six.moves.range(current, stop, step)]\n",
    "        elif isinstance(index, list) or isinstance(index, numpy.ndarray):\n",
    "            return [self.get_example_wrapper(i) for i in index]\n",
    "        else:\n",
    "            return self.get_example_wrapper(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of data points.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_example_wrapper(self, i):\n",
    "        \"\"\"Wrapper of `get_example`, to apply `transform` if necessary\"\"\"\n",
    "        example = self.get_example(i)\n",
    "        if self.transform:\n",
    "            example = self.transform(example)\n",
    "        return example\n",
    "\n",
    "    def get_example(self, i):\n",
    "        \"\"\"Returns the i-th example.\n",
    "\n",
    "        Implementations should override it. It should raise :class:`IndexError`\n",
    "        if the index is invalid.\n",
    "\n",
    "        Args:\n",
    "            i (int): The index of the example.\n",
    "\n",
    "        Returns:\n",
    "            The i-th example.\n",
    "\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-fruit",
   "metadata": {},
   "source": [
    "**UPDATE:** Here I mixup augmentation in the dataset. It makes interpolation of 2 images, with the label is also modified according to the mix ratio. mixup augmentation is expecially useful when the number of data is limited. Because it can make combination of any 2 images, instead of just using 1 image.\n",
    "\n",
    "I also added label smoothing feature. Sometimes it is difficult to learn label is 0, 1. Label smoothing changes its label 0 $\\rightarrow$ 0.01 & 1 $\\rightarrow$ 0.99. By smoothing the label the loss surface becomes more \"soft\", and sometimes model can learn well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-paint",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class VinbigdataTwoClassDataset(DatasetMixin):\n",
    "    def __init__(self, dataset_dicts, image_transform=None, transform=None, train: bool = True,\n",
    "                 mixup_prob: float = -1.0, label_smoothing: float = 0.0):\n",
    "        super(VinbigdataTwoClassDataset, self).__init__(transform=transform)\n",
    "        self.dataset_dicts = dataset_dicts\n",
    "        self.image_transform = image_transform\n",
    "        self.train = train\n",
    "        self.mixup_prob = mixup_prob\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def _get_single_example(self, i):\n",
    "        d = self.dataset_dicts[i]\n",
    "        filename = d[\"file_name\"]\n",
    "\n",
    "        img = cv2.imread(filename)\n",
    "        if self.image_transform:\n",
    "            img = self.image_transform(img)\n",
    "        img = torch.tensor(np.transpose(img, (2, 0, 1)).astype(np.float32))\n",
    "\n",
    "        if self.train:\n",
    "            label = int(len(d[\"annotations\"]) > 0)  # 0 normal, 1 abnormal\n",
    "            if self.label_smoothing > 0:\n",
    "                if label == 0:\n",
    "                    return img, float(label) + self.label_smoothing\n",
    "                else:\n",
    "                    return img, float(label) - self.label_smoothing\n",
    "            else:\n",
    "                return img, float(label)\n",
    "        else:\n",
    "            # Only return img\n",
    "            return img, None\n",
    "\n",
    "    def get_example(self, i):\n",
    "        img, label = self._get_single_example(i)\n",
    "        if self.mixup_prob > 0. and np.random.uniform() < self.mixup_prob:\n",
    "            j = np.random.randint(0, len(self.dataset_dicts))\n",
    "            p = np.random.uniform()\n",
    "            img2, label2 = self._get_single_example(j)\n",
    "            img = img * p + img2 * (1 - p)\n",
    "            if self.train:\n",
    "                label = label * p + label2 * (1 - p)\n",
    "\n",
    "        if self.train:\n",
    "            label_logit = torch.tensor([1 - label, label], dtype=torch.float32)\n",
    "            return img, label_logit\n",
    "        else:\n",
    "            # Only return img\n",
    "            return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-weight",
   "metadata": {},
   "source": [
    "Now creating the dataset is just easy as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dicts = get_vinbigdata_dicts(imgdir, train, debug=debug)\n",
    "dataset = VinbigdataTwoClassDataset(dataset_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-account",
   "metadata": {},
   "source": [
    "You can access each image and its label (0=Normal, 1=Abnormal) by just access dataset with index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-classics",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "img, label = dataset[index]\n",
    "plt.imshow(img.cpu().numpy().transpose((1, 2, 0)) / 255.)\n",
    "plt.title(f\"{index}-th image: label {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-caution",
   "metadata": {},
   "source": [
    "To run augmentation on this image, I will define `Transform` class which is applied each time the data is accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-geology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n",
    "class Transform:\n",
    "    def __init__(\n",
    "        self, hflip_prob: float = 0.5, ssr_prob: float = 0.5, random_bc_prob: float = 0.5\n",
    "    ):\n",
    "        self.transform = A.Compose(\n",
    "            [\n",
    "                A.HorizontalFlip(p=hflip_prob),\n",
    "                A.ShiftScaleRotate(\n",
    "                    shift_limit=0.0625, scale_limit=0.1, rotate_limit=10, p=ssr_prob\n",
    "                ),\n",
    "                A.RandomBrightnessContrast(p=random_bc_prob),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __call__(self, image):\n",
    "        image = self.transform(image=image)[\"image\"]\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-tooth",
   "metadata": {},
   "source": [
    "To use augmentation, you can just define dataset with the `Transform` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_dataset = VinbigdataTwoClassDataset(dataset_dicts, image_transform=Transform())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-modem",
   "metadata": {},
   "source": [
    "Let us visualize, looks good.\n",
    "\n",
    "You can see each image looks different (rotated, brightness is different etc...) even if it is generated from the same image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "\n",
    "n_images = 4\n",
    "\n",
    "fig, axes = plt.subplots(1, n_images, figsize=(16, 5))\n",
    "\n",
    "for i in range(n_images):\n",
    "    # Each time the data is accessed, the result is different due to random augmentation!\n",
    "    img, label = aug_dataset[index]\n",
    "    ax = axes[i]\n",
    "    ax.imshow(img.cpu().numpy().transpose((1, 2, 0)) / 255.)\n",
    "    ax.set_title(f\"{index}-th image: label {label}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-medication",
   "metadata": {},
   "source": [
    "Augmentation is very important hyperparameter to improve model's performance, and we want to experiment with various configurations.\n",
    "\n",
    "The `Transform` function is written below to support all the augmentations implemented in albumentations.\n",
    "\n",
    "We can specify `aug_kwargs` from external configuration inside `flag` as follows:\n",
    "\n",
    "~~~\n",
    "aug_kwargs:\n",
    "    HorizontalFlip: {\"p\": 0.5}\n",
    "    ShiftScaleRotate: {\"scale_limit\": 0.15, \"rotate_limit\": 10, \"p\": 0.5}\n",
    "    RandomBrightnessContrast: {\"p\": 0.5}\n",
    "    CoarseDropout: {\"max_holes\": 8, \"max_height\": 25, \"max_width\": 25, \"p\": 0.5}\n",
    "    Blur: {\"blur_limit\": [3, 7], \"p\": 0.5}\n",
    "    Downscale: {\"scale_min\": 0.25, \"scale_max\": 0.9, \"p\": 0.3}\n",
    "    RandomGamma: {\"gamma_limit\": [80, 120], \"p\": 0.6}\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "\n",
    "class Transform:\n",
    "    def __init__(self, aug_kwargs: Dict):\n",
    "        self.transform = A.Compose(\n",
    "            [getattr(A, name)(**kwargs) for name, kwargs in aug_kwargs.items()]\n",
    "        )\n",
    "\n",
    "    def __call__(self, image):\n",
    "        image = self.transform(image=image)[\"image\"]\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-childhood",
   "metadata": {},
   "source": [
    "## Defining the CNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-wildlife",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
